service: apache-kafka-sasl-scram

frameworkVersion: "3"
configValidationMode: error

provider:
  name: aws
  architecture: arm64
  region: ${env:AWS_REGION}
  stage: "dev"
  environment:
    SASL_SCRAM_USERNAME: ${env:SASL_SCRAM_USERNAME}
    SASL_SCRAM_PASSWORD: ${env:SASL_SCRAM_PASSWORD}
    KEY_PAIR_BUCKET_NAME: ${env:KEY_PAIR_BUCKET_NAME}
    KEY_PAIR_NAME: ${env:KEY_PAIR_NAME}
    STATE: ${env:STATE}
    COUNTRY: ${env:COUNTRY}
    ORGANIZATION_UNIT: ${env:ORGANIZATION_UNIT}
    CITY: ${env:CITY}

resources:
  Parameters: 
    KeyPairName: 
      Description: Ec2 Key Pair Name
      Default: '${env:KEY_PAIR_NAME}'
      Type: String
    LatestAmiId:
      Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
      Default: '/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64'
    InstanceType:
      Description: Apache Kafka EC2 instance type
      Type: String
      Default: t2.2xlarge
      AllowedValues:
        - t2.nano
        - t2.micro
        - t2.small
        - t2.large
        - t2.medium
        - t2.xlarge
        - t3.medium
        - t2.2xlarge
  Resources:
    Ec2RolePolicies: 
      Type: AWS::IAM::Policy
      Properties:
        PolicyName: Ec2RolePolicies
        PolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: "Allow"
              Action:
                - "s3:GetObject"
                - "s3:List*"
              Resource: "*"
            - Effect: "Allow"
              Action:
                - "cloudwatch:PutMetricData"
                - "ec2:DescribeVolumes"
                - "ec2:DescribeInstances"
                - "ec2:DescribeTags"
                - "logs:PutLogEvents"
                - "logs:DescribeLogStreams"
                - "logs:DescribeLogGroups"
                - "logs:CreateLogStream"
                - "logs:CreateLogGroup"
                - "ec2:DescribeLaunchTemplates"
                - "ec2:DescribeLaunchTemplateVersions"
              Resource: "*"
            - Effect: "Allow" 
              Action:
                - "ssm:GetParameter"
                - "ssm:PutParameter"
              Resource: "arn:aws:ssm:*:*:parameter/EC2-Custom-Metrics-*"
            - Effect: "Allow"
              Action:
                - "ssm:SendCommand"
                - "ssm:ListCommandInvocations"
                - "ssm:DescribeInstanceInformation"
              Resource: "*"
            - Effect: "Allow"
              Action: 
                - "iam:GetInstanceProfile"
                - "iam:SimulatePrincipalPolicy"
              Resource: "*"
        Roles:
          - { "Ref": "Ec2IAMRole" } 
    Ec2IAMRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
        ManagedPolicyArns:
          - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
          - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - arn:aws:iam::aws:policy/AmazonInspector2ManagedCispolicy
          - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        Path: /
        Policies: { 'Ref': 'AWS::NoValue' }
        RoleName: { 'Ref': 'AWS::NoValue' }
    Ec2IAMProfile:
      Type: AWS::IAM::InstanceProfile
      DependsOn: Ec2IAMRole
      Properties:
        Path: /
        Roles:
          - { 'Ref': 'Ec2IAMRole' }
    Ec2SecurityGroupSSH: 
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: allow SSH via static IP
        GroupName: ${self:provider.stage}-ec2-security-group-ssh
        SecurityGroupIngress:
          - IpProtocol: tcp
            FromPort: 22
            ToPort: 22
            CidrIp: 0.0.0.0/0
    BrokerSecurityGroup:
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: "Allow inbound traffic on port 9092"
        GroupName: ${self:provider.stage}-ec2-security-group-broker
        SecurityGroupIngress:
          - IpProtocol: tcp
            FromPort: 9092
            ToPort: 9092
            CidrIp: 0.0.0.0/0
    InterBrokerSecurityGroup:
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: "Allow inbound traffic on port 9092"
        GroupName: ${self:provider.stage}-ec2-security-group-inter-broker
        SecurityGroupIngress:
          - IpProtocol: tcp
            FromPort: 9091
            ToPort: 9091
            CidrIp: 0.0.0.0/0
    QuorumSecurityGroup:
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: "Allow inbound traffic on port 9092"
        GroupName: ${self:provider.stage}-ec2-security-group-quorom
        SecurityGroupIngress:
          - IpProtocol: tcp
            FromPort: 9093
            ToPort: 9093
            CidrIp: 0.0.0.0/0
    IcmpSecurityGroup:
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: "Allow outbound traffic on all ports"
        GroupName: ${self:provider.stage}-ec2-s-g-icmp
        SecurityGroupIngress:
          - IpProtocol: -1
            # FromPort: -1
            # ToPort: -1
            # CidrIp: 0.0.0.0/0
    LaunchTemplate:
      Type: 'AWS::EC2::LaunchTemplate'
      Properties:
        LaunchTemplateName: ${self:provider.stage}-KafkaEC2LaunchTemplate
        LaunchTemplateData:
          InstanceType: { 'Ref': 'InstanceType' }
          ImageId: { 'Ref': 'LatestAmiId' }
          SecurityGroupIds:
            - { 'Fn::GetAtt': ['Ec2SecurityGroupSSH', 'GroupId'] }
            - { 'Fn::GetAtt': ['BrokerSecurityGroup', 'GroupId'] }
            - { 'Fn::GetAtt': ['QuorumSecurityGroup', 'GroupId'] }
            - { 'Fn::GetAtt': ['InterBrokerSecurityGroup', 'GroupId'] }
            - { 'Fn::GetAtt': ['IcmpSecurityGroup', 'GroupId'] }
          KeyName: !Join [ "-", [ {"Ref": "KeyPairName"}, { 'Ref': 'AWS::Region' } ] ]
          IamInstanceProfile:
            Arn: { 'Fn::GetAtt': ['Ec2IAMProfile', 'Arn'] }
    KafkaKraftCluster14: 
      Type: "AWS::EC2::Instance"
      Properties:
        AvailabilityZone: { "Fn::Select": ["0", { "Fn::GetAZs": { "Ref": "AWS::Region" } }] }
        LaunchTemplate:
          LaunchTemplateId: { "Ref": "LaunchTemplate" }
          Version: { "Fn::GetAtt" : [ "LaunchTemplate", "DefaultVersionNumber" ] }
        Tags:
          - Key: Name
            Value: Kafka Kraft Instance
        UserData:
          Fn::Base64: !Sub 
          - |
            #!/bin/bash -u
            sudo yum update -y
            sudo yum install -y java-11-amazon-corretto
            sudo yum install -y git
            export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto
            export PATH=$PATH:$JAVA_HOME/bin
            wget https://downloads.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz
            tar xzf kafka_2.12-3.9.0.tgz
            sudo mv -f kafka_2.12-3.9.0 /opt
            sudo ln -s kafka_2.12-3.9.0 /opt/kafka
            sudo chown ec2-user:ec2-user /opt/kafka && sudo chmod u+s /opt/kafka
            #sudo rm -rf opt/kafka_2.12-3.9.0

            ls -l 
            ls -l /opt/kafka/
            # Generate TLS Certificate and Stores
            sudo mkdir /opt/kafka/config/kafka-ssl
            git clone https://github.com/confluentinc/confluent-platform-security-tools.git  /opt/kafka/config/kafka-ssl
            sudo chmod +x /opt/kafka/config/kafka-ssl/kafka-generate-ssl-automatic.sh
            cd /opt/kafka/config/kafka-ssl/
            echo COUNTRY=${country} >> /etc/environment
            echo STATE=${state} >> /etc/environment
            echo ORGANIZATION_UNIT=PX >> /etc/environment
            echo CITY=Johannesburg >> /etc/environment
            echo PASSWORD=${password} >> /etc/environment
            echo USERNAME=${username} >> /etc/environment
            sudo ./kafka-generate-ssl-automatic.sh
            ls -l
            cd ../../../../
            ls -l

            sudo touch /opt/kafka/scripts/kafka-format.sh
            sudo cat <<EOF > /opt/kafka/scripts/kafka-format.sh
            #!/bin/bash
            set -euo pipefail
            
            KAFKA_DIR="/opt/kafka"
            CONFIG="$KAFKA_DIR/config/server.properties"
            LOG_DIR="/opt/kafka/data/logs"
            
            # Only format if not already formatted
            if [ ! -f "$LOG_DIR/meta.properties" ]; then
              CLUSTER_ID=$(uuidgen)
              echo "Formatting Kafka KRaft storage with CLUSTER_ID=$CLUSTER_ID"
              "$KAFKA_DIR/bin/kafka-storage.sh" format --cluster-id "$CLUSTER_ID" --config "$CONFIG"
            else
              echo "Kafka storage already formatted."
            fi
            EOF
            sudo chmod +x /opt/kafka/scripts/kafka-format.sh
            
            sudo touch /opt/kafka/config/kraft/jaas.config
            sudo cat <<EOF > /opt/kafka/config/kraft/jaas.config
            KafkaServer {
                org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password};
            };
            EOF

            sudo touch /opt/kafka/config/kraft/admin.config
            sudo cat <<EOF > /opt/kafka/config/kraft/admin.config
            security.protocol=SASL_SSL
            ssl.truststore.location=/opt/kafka/config/kafka-ssl/truststore/kafka.truststore.jks
            ssl.truststore.password=${password}
            sasl.mechanism=SCRAM-SHA-256
            ssl.endpoint.identification.algorithm=
            sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password};
            EOF

            sudo mkdir -p /var/lib/kafka/logs
            sudo chmod -R 700 /var/lib/kafka
            sudo chown -R ec2-user:ec2-user /var/lib/kafka

            sudo sed -i s/num.partitions=1/num.partitions=8/ /opt/kafka/config/kraft/server.properties
            sudo sed -i s/log.dirs=\\/tmp\\/kraft-combined-logs/log.dirs=\\/var\\/bin\\/kafka\\/logs/ /opt/kafka/config/kraft/server.properties
      
            sudo sh -c 'cat << EOF >> /opt/kafka/config/kraft/server.properties
            sasl.enabled.mechanisms=SASL_PLAINTEXT,SCRAM-SHA-256,SCRAM-SHA-512,PLAIN,SASL_SSL
            sasl.mechanism.controller.protocol=SCRAM-SHA-256

            sasl.mechanism.inter.broker.protocol=SASL_SSL
            ssl.client.auth=required
            ssl.truststore.location=/opt/kafka/config/kafka-ssl/truststore/kafka.truststore.jks
            ssl.truststore.type=PKCS12
            ssl.truststore.password=${password}
            ssl.keystore.location=/opt/kafka/config/kafka-ssl/keystore/kafka.keystore.jks
            ssl.keystore.type=PKCS12
            ssl.keystore.password=${password}
            ssl.key.password=${password}
            ssl.endpoint.identification.algorithm=
            authorizer.class.name=org.apache.kafka.metadata.authorizer.StandardAuthorizer
            allow.everyone.if.no.acl.found=false
            super.users=User:admin
            delete.topic.enable=true
            default.replication.factor=2
            min.insync.replicas=2
            auto.create.topics.enable=true
            unclean.leader.election.enable=false
            log4j.logger.org.apache.kafka.common.network.SslTransportLayer=DEBUG
            log4j.logger.org.apache.kafka.common.security.ssl.SslFactory=DEBUG
            EOF'
            
            CLUSTER_ID=$(/opt/kafka/bin/kafka-storage.sh random-uuid)
            sudo /opt/kafka/bin/kafka-storage.sh format --config /opt/kafka/config/kraft/server.properties --cluster-id $CLUSTER_ID --add-scram SCRAM-SHA-256=[name=${username},password=${password}]
            echo $CLUSTER_ID;
            
            echo 'export KAFKA_HEAP_OPTS="-Xms1G -Xmx1G"' >> /etc/environment
            echo 'export KAFKA_OPTS="-Djava.security.auth.login.config=/opt/kafka/config/kraft/jaas.config"' >> /etc/environment
            echo 'export KAFKA_JVM_PERFORMANCE_OPTS="-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"' >> /etc/environment

            # Create Kafka service
            sudo cat <<EOF > /etc/systemd/system/kafka.service
            [Unit]
            Description=Kafka Service
            After=network-online.target
            Requires=network-online.target
             
            [Service]
            Type=simple
            User=ec2-user
            Restart=on-failure
            SyslogIdentifier=kafka
            Environment="KAFKA_HEAP_OPTS='-Xms1G -Xmx1G'"
            Environment="KAFKA_OPTS=-Djava.security.auth.login.config=/opt/kafka/config/kraft/jaas.config"
            Environment="KAFKA_JVM_PERFORMANCE_OPTS=-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"
            #ExecStartPre=sudo /opt/kafka/scripts/kafka-format.sh
            ExecStart=sudo /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/kraft/server.properties
            ExecStop=sudo /opt/kafka/bin/kafka-server-stop.sh /opt/kafka/config/kraft/server.properties
             
            [Install]
            WantedBy=multi-user.target
            EOF

            sudo sed -i 's/#ClientAliveInterval 0/ClientAliveInterval 300/' /etc/ssh/sshd_config
            sudo sed -i 's/#ClientAliveCountMax 3/ClientAliveCountMax 3/' /etc/ssh/sshd_config

            sudo systemctl daemon-reload
            sudo systemctl enable kafka
            sudo systemctl start kafka
            sudo systemctl status kafka

          - username: { "Fn::Sub": [ "${username}", { "username": "${env:SASL_SCRAM_USERNAME}" } ] }
            password: { "Fn::Sub": [ "${password}", { "password": "${env:SASL_SCRAM_PASSWORD}" } ] }
            state: { "Fn::Sub": [ "${state}", { "state": "${env:STATE}" } ] }
            country: { "Fn::Sub": [ "${country}", { "country": "${env:COUNTRY}" } ] }
            unit: { "Fn::Sub": [ "${unit}", { "unit": "${env:ORGANIZATION_UNIT}" } ] }
            city: { "Fn::Sub": [ "${city}", { "city": "${env:CITY}" } ] }
    # AKElasticIP:
    #   Type: AWS::EC2::EIP
    #   Properties:
    #     Domain: vpc
  
    # AKEIPAssociation:
    #   Type: AWS::EC2::EIPAssociation
    #   Properties:
    #     AllocationId: { 'Fn::GetAtt': ['AKElasticIP', 'AllocationId'] }
    #     InstanceId: { 'Ref': 'KafkaKraftCluster14' }

  Outputs:
    PublicIp:
      Description: Cluster Instance IP Address
      Value: { "Fn::GetAtt": ["KafkaKraftCluster14", "PublicIp"] }
      Export:
        Name: { "Fn::Sub": "${AWS::StackName}-ipaddress" }