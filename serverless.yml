service: apache-kafka-kraft-auth

frameworkVersion: "3"
configValidationMode: error

provider:
  name: aws
  architecture: arm64
  region: ${env:AWS_REGION}
  stage: "dev"
  environment:
    SASL_SCRAM_USERNAME: ${env:SASL_SCRAM_USERNAME}
    SASL_SCRAM_PASSWORD: ${env:SASL_SCRAM_PASSWORD}
    KEY_PAIR_BUCKET_NAME: ${env:KEY_PAIR_BUCKET_NAME}
    KEY_PAIR_NAME: ${env:KEY_PAIR_NAME}
    STATE: ${env:STATE}
    COUNTRY: ${env:COUNTRY}
    ORGANIZATION_UNIT: ${env:ORGANIZATION_UNIT}
    CITY: ${env:CITY}
    VPC_ID: ${env:VPC_ID}

resources:
  Parameters: 
    KeyPairName: 
      Description: EC2 Key Pair Name
      Default: '${env:KEY_PAIR_NAME}'
      Type: String
    LatestAmiId:
      Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
      Default: '/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64'
    InstanceType:
      Description: Apache Kafka EC2 instance type
      Type: String
      Default: t3.medium
      AllowedValues:
        - t2.nano
        - t2.micro
        - t2.small
        - t2.large
        - t2.medium
        - t3.medium
    MyVPC:
      Type: AWS::EC2::VPC::Id
      Default: ${env:VPC_ID}
  Resources:
    Ec2RolePolicies: 
      Type: AWS::IAM::Policy
      Properties:
        PolicyName: Ec2RolePolicies
        PolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: "Allow"
              Action:
                - "s3:GetObject"
                - "s3:List*"
              Resource: "*"
            - Effect: "Allow"
              Action:
                - "cloudwatch:PutMetricData"
                - "ec2:DescribeVolumes"
                - "ec2:DescribeInstances"
                - "ec2:DescribeTags"
                - "logs:PutLogEvents"
                - "logs:DescribeLogStreams"
                - "logs:DescribeLogGroups"
                - "logs:CreateLogStream"
                - "logs:CreateLogGroup"
                - "ec2:DescribeLaunchTemplates"
                - "ec2:DescribeLaunchTemplateVersions"
              Resource: "*"
            - Effect: "Allow" 
              Action:
                - "ssm:GetParameter"
                - "ssm:PutParameter"
              Resource: "arn:aws:ssm:*:*:parameter/EC2-Custom-Metrics-*"
            - Effect: "Allow"
              Action:
                - "ssm:SendCommand"
                - "ssm:ListCommandInvocations"
                - "ssm:DescribeInstanceInformation"
              Resource: "*"
            - Effect: "Allow"
              Action: 
                - "iam:GetInstanceProfile"
                - "iam:SimulatePrincipalPolicy"
              Resource: "*"
        Roles:
          - { "Ref": "Ec2IAMRole" } 
    Ec2IAMRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
        ManagedPolicyArns:
          - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
          - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - arn:aws:iam::aws:policy/AmazonInspector2ManagedCispolicy
          - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        Path: /
        Policies: { 'Ref': 'AWS::NoValue' }
        RoleName: { 'Ref': 'AWS::NoValue' }
    Ec2IAMProfile:
      Type: AWS::IAM::InstanceProfile
      DependsOn: Ec2IAMRole
      Properties:
        Path: /
        Roles:
          - { 'Ref': 'Ec2IAMRole' }
    # Ec2SecurityGroupSSH: 
    #   Type: AWS::EC2::SecurityGroup
    #   Properties:
    #     GroupDescription: allow SSH via static IP
    #     GroupName: ${self:provider.stage}-ec2-security-group-ssh
    #     # VpcId: {'Ref': 'MyVPC'}
    #     SecurityGroupIngress:
    #       - IpProtocol: tcp
    #         FromPort: 22
    #         ToPort: 22
    #         CidrIp: 0.0.0.0/0
    #     # SecurityGroupEgress:
    #     #   - IpProtocol: -1
    # BrokerSecurityGroup:
    #   Type: AWS::EC2::SecurityGroup
    #   Properties:
    #     GroupDescription: "Allow inbound traffic on port 9092"
    #     GroupName: ${self:provider.stage}-ec2-sg-sasl
    #     # VpcId: {'Ref': 'MyVPC'}
    #     SecurityGroupIngress:
    #       - IpProtocol: tcp
    #         FromPort: 9092
    #         ToPort: 9092
    #         CidrIp: 0.0.0.0/0
    #     # SecurityGroupEgress:
    #     #   - IpProtocol: -1
    # InterBrokerSecurityGroup:
    #   Type: AWS::EC2::SecurityGroup
    #   Properties:
    #     GroupDescription: "Allow inbound traffic on port 9093"
    #     GroupName: ${self:provider.stage}-ec2-sg-internal
    #     # VpcId: {'Ref': 'MyVPC'}
    #     SecurityGroupIngress:
    #       - IpProtocol: tcp
    #         FromPort: 9093
    #         ToPort: 9093
    #         CidrIp: 0.0.0.0/0
    #     # SecurityGroupEgress:
    #     #   - IpProtocol: -1
    # QuorumSecurityGroup:
    #   Type: AWS::EC2::SecurityGroup
    #   Properties:
    #     GroupDescription: "Allow inbound traffic on port 9094"
    #     GroupName: ${self:provider.stage}-ec2-sg-controller
    #     # VpcId: {'Ref': 'MyVPC'}
    #     SecurityGroupIngress:
    #       - IpProtocol: tcp
    #         FromPort: 9094
    #         ToPort: 9094
    #         CidrIp: 0.0.0.0/0
    #     # SecurityGroupEgress:
    #     #   - IpProtocol: -1
    # IcmpSecurityGroup:
    #   Type: AWS::EC2::SecurityGroup
    #   Properties:
    #     GroupDescription: "Allow outbound traffic on all ports"
    #     GroupName: ${self:provider.stage}-ec2-s-g-icmp
    #     # VpcId: {'Ref': 'MyVPC'}
    #     SecurityGroupEgress:
    #       - IpProtocol: -1
    MyKafkaSecurityGroup:
      Type: AWS::EC2::SecurityGroup
      Properties:
        GroupDescription: "Kafka Security Group"
        GroupName: ${self:provider.stage}-apache-kafka-security-group
        VpcId: !Ref MyVPC
        SecurityGroupIngress:
          - IpProtocol: tcp
            FromPort: '22'
            ToPort: '22'
            CidrIp: "0.0.0.0/0"  
          - IpProtocol: tcp
            FromPort: '9092'
            ToPort: '9092'
            CidrIp: "0.0.0.0/0"  # Allow external clients to connect
          - IpProtocol: tcp
            FromPort: '9093'
            ToPort: '9093'
            CidrIp: "0.0.0.0/0"  # Allow inter-broker communication
          - IpProtocol: tcp
            FromPort: '9094'  
            ToPort: '9094'
            CidrIp: "0.0.0.0/0"
    LaunchTemplate:
      Type: 'AWS::EC2::LaunchTemplate'
      Properties:
        LaunchTemplateName: ${self:provider.stage}-ApacheKafkaEC2LaunchTemplate
        LaunchTemplateData:
          InstanceType: { 'Ref': 'InstanceType' }
          ImageId: { 'Ref': 'LatestAmiId' }
          SecurityGroupIds:
            - { 'Fn::GetAtt': ['MyKafkaSecurityGroup', 'GroupId'] }
            # - { 'Fn::GetAtt': ['BrokerSecurityGroup', 'GroupId'] }
            # - { 'Fn::GetAtt': ['QuorumSecurityGroup', 'GroupId'] }
            # - { 'Fn::GetAtt': ['InterBrokerSecurityGroup', 'GroupId'] }
            # - { 'Fn::GetAtt': ['IcmpSecurityGroup', 'GroupId'] }
          KeyName: !Join [ "-", [ {"Ref": "KeyPairName"}, { 'Ref': 'AWS::Region' } ] ]
          IamInstanceProfile:
            Arn: { 'Fn::GetAtt': ['Ec2IAMProfile', 'Arn'] }
    KafkaKraftCluster62: 
      Type: "AWS::EC2::Instance"
      Properties:
        AvailabilityZone: { "Fn::Select": ["0", { "Fn::GetAZs": { "Ref": "AWS::Region" } }] }
        LaunchTemplate:
          LaunchTemplateId: { "Ref": "LaunchTemplate" }
          Version: { "Fn::GetAtt" : [ "LaunchTemplate", "DefaultVersionNumber" ] }
        Tags:
          - Key: Name
            Value: Apache Kafka Kraft Instance
        UserData:
          Fn::Base64: !Sub 
          - |
            #!/bin/bash -u
            sudo yum update -y
            sudo yum install -y java-11-amazon-corretto
            sudo yum install -y git
            sudo yum -y install telnet
            export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto
            export PATH=$PATH:$JAVA_HOME/bin
            wget https://downloads.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz
            tar xzf kafka_2.12-3.9.0.tgz
            sudo mv -f kafka_2.12-3.9.0 /opt
            sudo ln -s kafka_2.12-3.9.0 /opt/kafka
            sudo chown ec2-user:ec2-user /opt/kafka && sudo chmod u+s /opt/kafka

            ls -l 
            ls -l /opt/kafka/
            # Generate TLS Certificate and Stores
            sudo mkdir /opt/kafka/config/kafka-ssl
            git clone https://github.com/confluentinc/confluent-platform-security-tools.git  /opt/kafka/config/kafka-ssl
            sudo chmod +x /opt/kafka/config/kafka-ssl/kafka-generate-ssl-automatic.sh
            cd /opt/kafka/config/kafka-ssl/
            echo COUNTRY=${country} >> /etc/environment
            echo STATE=${state} >> /etc/environment
            echo ORGANIZATION_UNIT=PX >> /etc/environment
            echo CITY=Johannesburg >> /etc/environment
            echo PASSWORD=${password} >> /etc/environment
            echo USERNAME=${username} >> /etc/environment
            sudo ./kafka-generate-ssl-automatic.sh
            ls -l
            cd ../../../../
            ls -l

            CLUSTER_ID=$(/opt/kafka/bin/kafka-storage.sh random-uuid)
            sudo mkdir /opt/kafka/scripts
            sudo mkdir /opt/kafka/config/kraft/kraft-combined-logs

            sudo touch /opt/kafka/scripts/kafka-format.sh
            sudo cat <<EOF > /opt/kafka/scripts/kafka-format.sh
            #!/bin/bash
            set -euo pipefail
            echo "Formatting Kafka KRaft storage with CLUSTER_ID=$CLUSTER_ID"
            sudo /opt/kafka/bin/kafka-storage.sh format --config /opt/kafka/config/kraft/server.properties --cluster-id $CLUSTER_ID --add-scram SCRAM-SHA-256=[name=${username},password=${password}]
            EOF
            sudo chmod +x /opt/kafka/scripts/kafka-format.sh
            
            sudo touch /opt/kafka/config/kraft/jaas.config
            sudo cat <<EOF > /opt/kafka/config/kraft/jaas.config
            KafkaServer {
                org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password};
            };
            EOF

            sudo touch /opt/kafka/config/kraft/client.properties
            sudo cat <<EOF > /opt/kafka/config/kraft/client.properties
            security.protocol=SASL_SSL
            ssl.truststore.location=/opt/kafka/config/kafka-ssl/truststore/kafka.truststore.jks
            ssl.truststore.password=${password}
            sasl.mechanism=SCRAM-SHA-256
            sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password};
            EOF

            sudo touch /opt/kafka/config/kraft/log4j.properties
            sudo cat <<EOF > /opt/kafka/config/kraft/log4j.properties
            log4j.rootLogger=INFO, stdout, kafkaAppender
            log4j.appender.stdout=org.apache.log4j.ConsoleAppender
            log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
            log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n
            log4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppender
            log4j.appender.kafkaAppender.File=/var/log/kafka/server.log
            log4j.appender.kafkaAppender.MaxFileSize=100MB
            log4j.appender.kafkaAppender.MaxBackupIndex=10
            log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout
            log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n
            log4j.logger.org.apache.kafka.metadata=INFO
            log4j.logger.org.apache.kafka.raft=INFO
            log4j.logger.org.apache.kafka.controller=INFO
            log4j.logger.org.apache.kafka.quorum=DEBUG
            EOF

            sudo mkdir -p /var/lib/kafka/logs
            sudo chmod -R 700 /var/lib/kafka
            sudo chown -R ec2-user:ec2-user /var/lib/kafka

            sudo sed -i s/num.partitions=1/num.partitions=8/ /opt/kafka/config/kraft/server.properties
            sudo sed -i s/log.dirs=\\/tmp\\/kraft-combined-logs/log.dirs=\\/opt\\/kafka\\/config\\/kraft\\/kraft-combined-logs/ /opt/kafka/config/kraft/server.properties
      
            sudo sh -c 'cat << EOF >> /opt/kafka/config/kraft/server.properties
            sasl.enabled.mechanisms=SCRAM-SHA-256
            sasl.mechanism.controller.protocol=SCRAM-SHA-256
            #security.inter.broker.protocol=SASL_SSL 
            sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256
            # sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password};
            # listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password} user_admin=${password};
            # listener.name.controller.sasl.enabled.mechanisms=SCRAM-SHA-256
            # listener.name.broker.sasl.enabled.mechanisms=SCRAM-SHA-256
            # listener.name.controller.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password} user_admin=${password};
            # listener.name.broker.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=${username} password=${password} user_admin=${password};
            # listener.name.controller.ssl.client.auth=required
            # listener.name.broker.ssl.client.auth=required
            ssl.client.auth=required
            ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1
            ssl.truststore.location=/opt/kafka/config/kafka-ssl/truststore/kafka.truststore.jks
            ssl.truststore.type=PKCS12
            ssl.truststore.password=${password}
            ssl.keystore.location=/opt/kafka/config/kafka-ssl/keystore/kafka.keystore.jks
            ssl.keystore.type=PKCS12
            ssl.keystore.password=${password}
            ssl.key.password=${password}
            ssl.endpoint.identification.algorithm=https
            authorizer.class.name=org.apache.kafka.metadata.authorizer.StandardAuthorizer
            #authorizer.class.name=kafka.security.authorizer.AclAuthorizer
            allow.everyone.if.no.acl.found=true
            super.users=User:admin;User:controller;User:broker;User:producer;User:consumer
            delete.topic.enable=true
            default.replication.factor=2
            min.insync.replicas=2
            auto.create.topics.enable=true
            unclean.leader.election.enable=false
            log4j.logger.org.apache.kafka.common.network.SslTransportLayer=DEBUG
            log4j.logger.org.apache.kafka.common.security.ssl.SslFactory=DEBUG
            EOF'
            
            echo 'export KAFKA_HEAP_OPTS="-Xms1G -Xmx1G"' >> /etc/environment
            echo 'export KAFKA_OPTS="-Djava.security.auth.login.config=/opt/kafka/config/kraft/jaas.config"' >> /etc/environment
            echo 'export KAFKA_JVM_PERFORMANCE_OPTS="-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"' >> /etc/environment
            echo 'KAFKA_LOG4J_OPTS="-Dlog4j.configuration=file:/opt/kafka/config/kraft/log4j.properties"' >> /etc/environment

            # Create Kafka service
            sudo cat <<EOF > /etc/systemd/system/kafka.service
            [Unit]
            Description=Kafka Service
            After=network-online.target
            Requires=network-online.target
             
            [Service]
            Type=simple
            User=ec2-user
            Restart=on-failure
            SyslogIdentifier=kafka
            Environment="KAFKA_HEAP_OPTS='-Xms1G -Xmx1G'"
            Environment="CLUSTER_ID=$CLUSTER_ID"
            Environment="KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:/opt/kafka/config/kraft/log4j.properties"
            Environment="KAFKA_OPTS=-Djava.security.auth.login.config=/opt/kafka/config/kraft/jaas.config"
            Environment="KAFKA_JVM_PERFORMANCE_OPTS=-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"
            ExecStartPre=sudo /opt/kafka/scripts/kafka-format.sh
            ExecStart=sudo /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/kraft/server.properties
            ExecStop=sudo /opt/kafka/bin/kafka-server-stop.sh /opt/kafka/config/kraft/server.properties
             
            [Install]
            WantedBy=multi-user.target
            EOF

            sudo sed -i 's/#ClientAliveInterval 0/ClientAliveInterval 300/' /etc/ssh/sshd_config
            sudo sed -i 's/#ClientAliveCountMax 3/ClientAliveCountMax 3/' /etc/ssh/sshd_config

            sudo systemctl daemon-reload
            # sudo systemctl enable kafka
            # sudo systemctl start kafka
            # sudo systemctl status kafka

          - username: { "Fn::Sub": [ "${username}", { "username": "${env:SASL_SCRAM_USERNAME}" } ] }
            password: { "Fn::Sub": [ "${password}", { "password": "${env:SASL_SCRAM_PASSWORD}" } ] }
            state: { "Fn::Sub": [ "${state}", { "state": "${env:STATE}" } ] }
            country: { "Fn::Sub": [ "${country}", { "country": "${env:COUNTRY}" } ] }
            unit: { "Fn::Sub": [ "${unit}", { "unit": "${env:ORGANIZATION_UNIT}" } ] }
            city: { "Fn::Sub": [ "${city}", { "city": "${env:CITY}" } ] }
    # AKElasticIP:
    #   Type: AWS::EC2::EIP
    #   Properties:
    #     Domain: vpc
  
    # AKEIPAssociation:
    #   Type: AWS::EC2::EIPAssociation
    #   Properties:
    #     AllocationId: { 'Fn::GetAtt': ['AKElasticIP', 'AllocationId'] }
    #     InstanceId: { 'Ref': 'KafkaKraftCluster14' }

  Outputs:
    PublicIp:
      Description: Cluster Instance IP Address
      Value: { "Fn::GetAtt": ["KafkaKraftCluster62", "PublicIp"] }
      Export:
        Name: { "Fn::Sub": "${AWS::StackName}-ipaddress" }